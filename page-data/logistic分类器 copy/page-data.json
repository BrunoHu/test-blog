{"componentChunkName":"component---src-templates-blog-post-js","path":"/logistic分类器 copy/","result":{"data":{"site":{"id":"Site","siteMetadata":{"title":"Gatsby Starter Blog"}},"markdownRemark":{"id":"a817166e-6ac9-50f7-bef8-37049632c460","excerpt":"准备 现在我们来到了分类器，首先我们考虑一个最简单的二元分类器，抽象成只判断对错。那么，换句话说我们希望我们得到的结果是离散的，但是我们前面的函数都是线性的，显然不能很好的处理这一类问题，所以，我们得新引进一个函数来把连续的结果处理成离散的效果，轮到logistic函数（又称sigmoid…","html":"<h3>准备</h3>\n<p>现在我们来到了分类器，首先我们考虑一个最简单的二元分类器，抽象成只判断对错。那么，换句话说我们希望我们得到的结果是离散的，但是我们前面的函数都是线性的，显然不能很好的处理这一类问题，所以，我们得新引进一个函数来把连续的结果处理成离散的效果，轮到logistic函数（又称sigmoid函数）登场了</p>\n<p>$g(x)=\\frac{1}{1+e^{-x}}$</p>\n<p>图像如下，很明显可以看出越靠近零点斜率变化越大，函数在0点附近急剧的变化到0和1，这正是两个非常适于数学处理的离散值。\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/sigmoid.png\"></p>\n<p>而基于线性回归，我们可以用一条线（或者说是超平面）来分割点集来得到分类的效果，那么我们可以通过刚才介绍的logistic函数来的到离散化的反馈结果来进行优化。那么我们的预测函数$h_\\theta (x)$如下</p>\n<p>$h_\\theta (x)=g(\\theta^T x) = \\frac{1}{1+e^{-\\theta^T x}}$</p>\n<p>那我们怎么来对$\\theta$进行迭代优化呢？线性回归可以通过极大似然估计，这个自然也是ok的，那么，我们也来估计一下。</p>\n<h3>概率意义</h3>\n<p>根据$h_\\theta (x)$的意义，我们能够得到下面的等式\n$P(y=1|x;\\theta)=h_\\theta(x)$\n$P(y=0|x;\\theta)=1-h_\\theta(x)$\n那么，综合一下得到一个适合数学分析的等式\n$P(y|x;\\theta) = h_\\theta(x)^y (1-h(x))^{1-y}$</p>\n<p>同样，我们求极大似然估计\n$L(\\theta) = \\prod_{i=1}^m h_\\theta (x^{(i)})^{y^{(i)}} (1-h(x^{(i)}))^{1-y^{(i)}}$\n$l(\\theta) = log(L(\\theta)) = \\sum_{i=1}^m y^{(i)}log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))$</p>\n<p>我们要使$l(\\theta)$最大，那么自然而然我们可以用到梯度上升法，那么，每次的迭代就是这样</p>\n<p>$\\theta := \\theta + \\alpha \\nabla_\\theta l(\\theta)$</p>\n<p>现在的问题就是怎么求$l(\\theta)$关于$\\theta$的梯度</p>\n<p>首先，我们先求</p>\n$$\n\\frac{\\partial h_\\theta (x^{(i)})}{\\partial \\theta_j} &=\n-(\\frac{1}{1+e^{-\\theta^T x^{(i)}}})^2 \\bullet e^{-\\theta^T x^{(i)}} \\bullet (-x_j^{(i)}) \\\\\n&= \\frac{1}{1+e^{-\\theta^T x^{(i)}}} \\bullet \\frac{e^{-\\theta^T x^{(i)}}}{1+e^{-\\theta^T x^{(i)}}} \\bullet x_j^{(i)} \\\\\n&= h_\\theta (x^{(i)})(1-h_\\theta (x^{(i)})) x_j^{(i)}\n\\end{align}$$\n\n那么，我们带入可以求得\n$$\n<p>&#x26; \\frac{\\partial}{\\partial \\theta<em>j} \\sum</em>{i=1}^m y^{(i)}logh<em>\\theta(x^{(i)}) + (1-y^{(i)})log(1- h</em>\\theta(x^{(i)})) \\\n&#x26;= \\sum<em>{i=1}^m \\frac{1}{h</em>\\theta (x^{(i)})} \\bullet \\frac{\\partial h<em>\\theta (x^{(i)})}{\\partial \\theta</em>j} +\n(1-y^{(i)}) \\bullet \\frac{1}{1-h<em>\\theta (x^{(i)})} \\bullet \\frac{\\partial (1-h</em>\\theta (x^{(i)}))}{\\partial \\theta<em>j} \\\n&#x26;= \\sum</em>{i=1}^m y^{(i)}(1-h<em>\\theta(x^{(i)}))x</em>j^{(i)} - (1-y^{(i)})h<em>\\theta(x^{(i)})x</em>j^{(i)} \\\n&#x26;= \\sum<em>{i=1}^m (y^{(i)} - h</em>\\theta(x^{(i)}))x_j^{(i)}\n\\end{align}$$</p>\n<h3>结果</h3>\n<p>最终我们得到了$\\theta$迭代式\n$\\theta_j := \\theta_j - \\alpha \\sum_{i=1}^m \\left[ h_\\theta (x^{(i)}) - y^{(i)}\\right] x_j^{(i)}$</p>\n<p>自然，我们也可以和线性回归的方法一样用随机梯度法</p>\n<p>$\\theta_j := \\theta_j - \\alpha \\left[h_\\theta (x^{(i)}) - y^{(i)} \\right] x_j^{(i)}$</p>\n<h3>有图有真相</h3>\n<p>左上图是不停迭代的分类器，右上是原始的分类数据，下方是每次生成的分类器的误差，数据经过了规整。</p>\n<p>突然发现两类数据有超平面可以完全分开的时候，$\\alpha$取多少都无所谓，原因不明，待思考~\n$\\alpha = 1$\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/logistic_1.png\">\n$\\alpha = 20$\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/logistic_20.png\">\n$\\alpha = 100$\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/logistic_100.png\"></p>\n<p>但是当不能完全分开的时候，取得alpha过大的话会在极值处徘徊无法收敛~</p>\n<p>$\\alpha=1,\\alpha=10$的时候效果还不错，都能收敛\n$\\alpha = 1$\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/logistic_1_mix.png\">\n$\\alpha = 10$\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/logistic_10_mix.png\">\n$\\alpha=100$的时候就已经无法收敛了\n$\\alpha=100$\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/logistic_100_mix.png\"></p>\n<p>还送上一个彩蛋，$\\alpha=1$,迭代10000次的情况，最后误差反而上升了！\n<img src=\"http://7xl294.com1.z0.glb.clouddn.com/logistic_10000.png\"></p>","frontmatter":{"title":"logistic分类器","date":"June 17, 2016","description":"desc","tags":["math","machine learning","algorithm"]}}},"pageContext":{"slug":"/logistic分类器 copy/","previous":{"fields":{"slug":"/leetcode记/"},"frontmatter":{"title":"leetcode记（持续更新中）"}},"next":{"fields":{"slug":"/logistic分类器/"},"frontmatter":{"title":"logistic分类器"}}}}}